# -*- coding: utf-8 -*-
"""Using_Gradient Boosting Classifier_ Course Project 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IaNNypAnjmwq6oaDCAtcPWOOYj0fsISC
"""

# ======================================================================
# CELL 1: IMPORT LIBRARIES
# ======================================================================
# Run this cell first to import all necessary packages

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier  # <--- Gradient Boosting Model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

print("Libraries imported successfully.")

# ======================================================================
# CELL 2: LOAD DATA & DEFINE CONSTANTS
# ======================================================================
# Make sure you have uploaded 'train.csv' and 'test.csv' to Colab!

# Define a fixed random state for reproducibility (Rule 9)
RANDOM_SEED = 42

print("Step 1: Loading data...")
try:
    train_df = pd.read_csv("train.csv")
    test_df = pd.read_csv("test.csv")
except FileNotFoundError as e:
    print(f"Error: {e}")
    print("Please make sure you have uploaded 'train.csv' and 'test.csv' to your Colab session.")
    raise

print("Data loaded successfully.")
print(f"Training data shape: {train_df.shape}")
print(f"Test data shape: {test_df.shape}")

# ======================================================================
# CELL 3: EXPLORATORY DATA ANALYSIS (EDA)
# ======================================================================
# This is the new cell for EDA, as required by Rule 4

print("\nStep 2: Performing Exploratory Data Analysis (EDA)...")

# 1. Basic Information
print("\n--- Data Info ---")
train_df.info()

print("\n--- Numerical Statistics ---")
print(train_df.describe())

# 2. Target Variable Distribution (WeightCategory)
print("\n--- Plotting Target Variable Distribution ---")
plt.figure(figsize=(10, 6))
sns.countplot(x='WeightCategory', data=train_df, order=train_df['WeightCategory'].value_counts().index)
plt.title('Distribution of WeightCategory (Target Variable)')
plt.xlabel('Weight Category')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('eda_target_distribution.png')
print("Saved plot 'eda_target_distribution.png'")

# 3. Key Feature Relationships
print("\n--- Plotting Key Feature Relationships ---")

# Define category order for logical plots
category_order = [
    'Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I',
    'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'
]

# Plot 1: Weight vs. Category
plt.figure(figsize=(12, 7))
sns.boxplot(x='WeightCategory', y='Weight', data=train_df, order=category_order)
plt.title('Weight Distribution by Weight Category')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('eda_weight_vs_category.png')
print("Saved plot 'eda_weight_vs_category.png'")

# Plot 2: Transportation (MTRANS) vs. Category
plt.figure(figsize=(14, 8))
sns.countplot(x='WeightCategory', hue='MTRANS', data=train_df, order=category_order)
plt.title('Transportation Mode by Weight Category')
plt.xticks(rotation=45)
plt.legend(title='Transportation', loc='upper left')
plt.tight_layout()
plt.savefig('eda_mtrans_vs_category.png')
print("Saved plot 'eda_mtrans_vs_category.png'")

# Plot 3: Family History vs. Category
plt.figure(figsize=(12, 7))
sns.countplot(x='WeightCategory', hue='family_history_with_overweight', data=train_df, order=category_order)
plt.title('Family History of Overweight by Weight Category')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('eda_familyhistory_vs_category.png')
print(f"Saved plot 'eda_familyhistory_vs_category.png'")
print("\nEDA complete. Check the file browser for saved plots.")

# ======================================================================
# CELL 4: DATA PREPROCESSING
# ======================================================================
# This cell will convert all data to a model-ready, numerical format

print("\nStep 3: Preprocessing data...")

# 3a. Define Features (X) and Target (y)
X = train_df.drop(['id', 'WeightCategory'], axis=1)
y = train_df['WeightCategory']
X_test = test_df.drop('id', axis=1)
test_ids = test_df['id'] # Save test IDs for submission

# 3b. Encode the Target Variable (y)
le = LabelEncoder()
y_encoded = le.fit_transform(y)
print(f"Target variable 'WeightCategory' encoded.")

# 3c. Identify Feature Types
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

# Create copies to work on
X_processed = X.copy()
X_test_processed = X_test.copy()

# 3d. Encode Categorical Features
binary_features = []
multi_class_features = []

for col in categorical_features:
    unique_vals = pd.concat([X_processed[col], X_test_processed[col]]).astype(str).unique()
    if len(unique_vals) == 2:
        binary_features.append(col)
    else:
        multi_class_features.append(col)

# Apply LabelEncoder to binary features
for col in binary_features:
    le_col = LabelEncoder()
    all_categories = pd.concat([X_processed[col], X_test_processed[col]]).astype(str).unique()
    le_col.fit(all_categories)
    X_processed[col] = le_col.transform(X_processed[col].astype(str))
    X_test_processed[col] = le_col.transform(X_test_processed[col].astype(str))

print("Binary features encoded.")

# Apply One-Hot Encoding to multi-class features
X_processed = pd.get_dummies(X_processed, columns=multi_class_features)
X_test_processed = pd.get_dummies(X_test_processed, columns=multi_class_features)

# Align columns - This is CRUCIAL
X_processed, X_test_processed = X_processed.align(X_test_processed, join='inner', axis=1, fill_value=0)
print("Multi-class features encoded and columns aligned.")

# 3e. Scale Numerical Features
scaler = StandardScaler()
X_processed[numerical_features] = scaler.fit_transform(X_processed[numerical_features])
X_test_processed[numerical_features] = scaler.transform(X_test_processed[numerical_features])
print("Numerical features scaled.")
print("Preprocessing complete.")

# ======================================================================
# CELL 5: CREATE TRAINING & VALIDATION SETS
# ======================================================================
# We split our training data so we can test our model internally

print("\nStep 4: Splitting data into training and validation sets...")

X_train, X_val, y_train, y_val = train_test_split(
    X_processed,
    y_encoded,
    test_size=0.2, # 20% for validation
    random_state=RANDOM_SEED,
    stratify=y_encoded # Ensures balanced classes in train/val sets
)

print(f"X_train shape: {X_train.shape}")
print(f"X_val shape: {X_val.shape}")
print("Data splitting complete.")

# ======================================================================
# CELL 6: TRAIN GRADIENT BOOSTING MODEL
# ======================================================================
# Now we train our model on the training data

print("\nStep 5: Training Gradient Boosting model...")
print("This may take 20-30 seconds...")

model_gb = GradientBoostingClassifier(random_state=RANDOM_SEED)

model_gb.fit(X_train, y_train)

print("Gradient Boosting model trained successfully.")

# ======================================================================
# CELL 7: VALIDATE MODEL & VISUALIZE RESULTS
# ======================================================================
# This is where we see how well our model performed

print("\nStep 6: Validating model performance and visualizing results...")

y_pred_val = model_gb.predict(X_val)

# 1. Get Accuracy Score
val_accuracy = accuracy_score(y_val, y_pred_val)
print(f"\n--- VALIDATION ACCURACY (Gradient Boosting): {val_accuracy * 100:.2f}% ---")

# 2. Get the Classification Report
print("\n--- Classification Report (Gradient Boosting) ---")
print(classification_report(y_val, y_pred_val, target_names=le.classes_))

# 3. Get and Plot the Confusion Matrix
print("\n--- Plotting Confusion Matrix (Gradient Boosting) ---")
cm = confusion_matrix(y_val, y_pred_val)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title('Validation Confusion Matrix - Gradient Boosting', fontsize=16)
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
matrix_filename = 'gradient_boosting_confusion_matrix.png'
plt.savefig(matrix_filename)
print(f"Confusion matrix saved as '{matrix_filename}'")

# ======================================================================
# CELL 8: TRAIN FINAL MODEL & CREATE SUBMISSION
# ======================================================================
# We now train on 100% of the data and predict on the test set

print("\nStep 7: Training final model and creating submission file...")

final_model = GradientBoostingClassifier(random_state=RANDOM_SEED)

print("Training final model on 100% of the training data...")
final_model.fit(X_processed, y_encoded)

print("Making predictions on the test set...")
test_predictions_encoded = final_model.predict(X_test_processed)
test_predictions_labels = le.inverse_transform(test_predictions_encoded)
print("Predictions decoded.")

# Create the submission DataFrame
submission_df = pd.DataFrame({'id': test_ids, 'WeightCategory': test_predictions_labels})
submission_filename = 'submission_gradient_boosting.csv'
submission_df.to_csv(submission_filename, index=False)

print(f"\n--- Project Complete (Gradient Boosting) ---")
print(f"Submission file saved as: {submission_filename}")
print(submission_df.head())